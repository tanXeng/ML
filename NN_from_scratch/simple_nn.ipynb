{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network\n",
    "This neural network will be built with numpy and have only have 1 hidden layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the relevant modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST dataset\n",
    "The datasets were taken from https://www.kaggle.com/competitions/digit-recognizer/data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42000 entries, 0 to 41999\n",
      "Columns: 785 entries, label to pixel783\n",
      "dtypes: int64(785)\n",
      "memory usage: 251.5 MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values:  0\n"
     ]
    }
   ],
   "source": [
    "print('Number of null values: ', df.isna().sum().sum()) # checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    train_size=0.8,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['label']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df, val_df = train_test_split(\n",
    "    test_df,\n",
    "    train_size=0.5,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=test_df['label']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_X = val_df.drop('label', axis=1).to_numpy().T / 255.0 # normalising\n",
    "val_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = val_df['label'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train_df.drop('label', axis=1).to_numpy().T / 255.0 # normalising\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 6, ..., 1, 8, 0], dtype=int64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = train_df['label'].to_numpy()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33600"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration\n",
    "Not much needed here, but let's just see what we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    4684\n",
       "7    4401\n",
       "3    4351\n",
       "9    4188\n",
       "2    4177\n",
       "6    4137\n",
       "0    4132\n",
       "4    4072\n",
       "8    4063\n",
       "5    3795\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a relatively balanced dataset so that's good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "Besides the output layer, these functions are useful because they introduce non-linearity into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid\n",
    "The sigmoid function is defined as:\n",
    "\n",
    "$$\n",
    "    \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Advantages:\n",
    "- The sigmoid function is differentiable and smooth, making it suitable for gradient descent.\n",
    "- Outputs between 0 and 1 making it useful for binary classification.\n",
    "\n",
    "Disadvantages:\n",
    "- Can suffer from vanishing gradient, if inputs to the function get too extreme or network is too deep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def dsigmoid_dz(z):\n",
    "    return np.exp(-z) / ((1 + np.exp(-z)) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh\n",
    "The tanh function is defined as:\n",
    "\n",
    "$$\n",
    "\\tanh(z) = \\displaystyle\\frac{e^z - e^{-z}}{e^z + e^{-z}}\n",
    "$$\n",
    "\n",
    "Advantages:\n",
    "- The tanh function is differentiable and smooth, making it suitable for gradient descent.\n",
    "- The tanh function is zero centered with steeper gradients, which can lead to faster convergence. As such, it is often prefered over the sigmoid function in the hidden layers.\n",
    "\n",
    "Disadvantages:\n",
    "- Can suffer from vanishing gradient, if inputs to the function get too extreme or network is too deep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def dtanh_dz(z):\n",
    "    return 1 - np.tanh(z) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU\n",
    "The ReLU is defined as:\n",
    "\n",
    "$$\n",
    "\\text{ReLU(z)} =\n",
    "\\begin{cases} \n",
    "z, & \\text{if } z > 0 \\\\\n",
    "0, & \\text{if } z \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Advantages:\n",
    "- The ReLU helps prevent the vanishing gradient problem.\n",
    "\n",
    "Disadvantages:\n",
    "- Can suffer from dying ReLU, where some neurons can become inactive, always outputting 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def dReLU_dz(z):\n",
    "    return (z > 0).astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the dying ReLU problem, we can use the leaky ReLU that is defined as:\n",
    "\n",
    "$$\n",
    "\\text{LeakyReLU}(z) =\n",
    "\\begin{cases} \n",
    "    z, & z > 0 \\\\\n",
    "    0.01 z, & z \\leq 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leakyReLU(z):\n",
    "    return np.maximum(0.01 * z, z)\n",
    "\n",
    "def dleakyReLU_dz(z):\n",
    "    return (z > 0).astype(np.float64) + 0.01 * (z <= 0).astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "For multi-class classification, we need a function to make sense of the output of the neural network. Hence, we present to you the softmax function:\n",
    "\n",
    "$$\n",
    "\\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{N} e^{z_j}}\n",
    "$$\n",
    "\n",
    "where $z_i$ is the $\\text{i} th$ output of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=0, keepdims=True)) # np.max(z, axis=0, keepdims=True) is deducted to keep numerical stability but it is mathematically the same\n",
    "    return exp_z / exp_z.sum(axis=0, keepdims=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot encoding with optional label smoothing\n",
    "In order to do back propagation, we need the labels to be one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for labels that are not digits 0-9, this function can easily be adapted\n",
    "def oneHot(y, smoothing_param=0):\n",
    "    one_hot_matrix = np.full((len(y), 10), smoothing_param / 10)\n",
    "\n",
    "    for i, label in enumerate(y):\n",
    "        one_hot_matrix[i][label] = 1 - smoothing_param\n",
    "    \n",
    "    return one_hot_matrix.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss\n",
    "This is the loss function commonly used for multi-class classification. For a given set of features, it is defined as:\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "where $y_i$ is the true probability for class $i$, and $\\hat{y}_i$ is the model's predicted probability for class $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropyLoss(y_hat, y, epsilon=1e-10):\n",
    "    return -np.sum(y * np.log(y_hat + epsilon)) / y.shape[1]\n",
    "\n",
    "def dCE_dz(y_hat, y):\n",
    "    return y_hat - y # it simplified very nicely!    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foward and backward propagation\n",
    "Activation functions and number of the layers can be modified here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W1, W2):\n",
    "    bias_row = np.ones(X.shape[1])\n",
    "\n",
    "    # input layer\n",
    "    z0 = X\n",
    "    z0 = np.vstack([z0, bias_row]) # for the bias \n",
    "\n",
    "    # hidden layer 1\n",
    "    z1 = W1.dot(z0)\n",
    "    a1 = ReLU(z1)\n",
    "    a1 = np.vstack([a1, bias_row]) # for the bias \n",
    "\n",
    "    # output layer\n",
    "    z2 = W2.dot(a1)\n",
    "    a2 = softmax(z2)\n",
    "    \n",
    "    return z0, z1, a1, z2, a2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(z0, W1, z1, a1, W2, z2, a2, y):\n",
    "    n = y.shape[1] # number of samples\n",
    "\n",
    "    # output layer\n",
    "    dL_dz2 = dCE_dz(a2, y)\n",
    "    dL_dW2 = dL_dz2.dot(a1.T) / n\n",
    "\n",
    "    # hidden layer 1\n",
    "    dL_da1 = W2.T.dot(dL_dz2)\n",
    "    dL_da1 = dL_da1[:-1] # removing the bias row\n",
    "    da1_dz1 = dReLU_dz(z1)\n",
    "    dL_dz1 = dL_da1 * da1_dz1\n",
    "    dL_dW1 = dL_dz1.dot(z0.T) / n\n",
    "    \n",
    "    return dL_dW1, dL_dW2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "We will use batch gradient descent, since our dataset is not too large. Other options include mini batch gradient descent, if the dataset is too large to fit into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_grad_descent(W1, dL_dW1, W2, dL_dW2, lr):\n",
    "    W1 -= (lr * dL_dW1)\n",
    "    W2 -= (lr * dL_dW2)\n",
    "    return W1, W2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predicted_labels, true_labels):\n",
    "    return np.sum(predicted_labels == true_labels) / len(true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to get prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W1, W2):\n",
    "    z0, z1, a1, z2, a2  = forward(X, W1, W2)\n",
    "    return np.argmax(a2, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(W1, W2, X, labels, val_X, val_labels, num_epochs, lr=0.1, patience=10, smoothing_param=0):\n",
    "\n",
    "    original_patience = patience\n",
    "\n",
    "    # one-hot encode labels\n",
    "    y = oneHot(labels, smoothing_param=smoothing_param)\n",
    "    val_y = oneHot(val_labels)\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    # training loop\n",
    "    for i in range(num_epochs):\n",
    "\n",
    "        # forward propagation\n",
    "        z0, z1, a1, z2, a2 = forward(X, W1, W2)\n",
    "\n",
    "        # print the training loss\n",
    "        training_loss = crossEntropyLoss(a2, y)\n",
    "\n",
    "        # validate\n",
    "        val_predictions = forward(val_X, W1, W2)[-1]\n",
    "        validation_loss = crossEntropyLoss(val_predictions, val_y)\n",
    "\n",
    "        # print the training loss\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('epoch: ', i + 1)\n",
    "            print('     training loss: ', training_loss)\n",
    "            print('     validation loss: ', validation_loss)\n",
    "\n",
    "        # only save the model when it validation loss improves\n",
    "        if validation_loss <= best_val_loss:\n",
    "            best_val_loss = validation_loss\n",
    "            best_W1, best_W2 = W1, W2\n",
    "            patience = original_patience\n",
    "\n",
    "        else:\n",
    "            print(\"     Bullocks! I think I'm overfitting...\")\n",
    "            patience -= 1\n",
    "\n",
    "            # early stopping\n",
    "            if patience <= 0:\n",
    "                print(\"     I'm definitely overfitted. Stopping training...\")\n",
    "                break\n",
    "\n",
    "        # back propagation\n",
    "        dL_dW1, dL_dW2 = backward(z0, W1, z1, a1, W2, z2, a2, y)\n",
    "\n",
    "        # update weights\n",
    "        W1, W2 = batch_grad_descent(W1, dL_dW1, W2, dL_dW2, lr)\n",
    "\n",
    "    return best_W1, best_W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 10\u001b[0m\n\u001b[0;32m      5\u001b[0m W2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m10\u001b[39m, hidden_units \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m129\u001b[39m) \u001b[38;5;66;03m# note that to account for the bias we use (10, 11) instead of (10, 10)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[1;32m---> 10\u001b[0m W1, W2 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmoothing_param\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[63], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(W1, W2, X, labels, val_X, val_labels, num_epochs, lr, patience, smoothing_param)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# training loop\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# forward propagation\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     z0, z1, a1, z2, a2 \u001b[38;5;241m=\u001b[39m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# print the training loss\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     training_loss \u001b[38;5;241m=\u001b[39m crossEntropyLoss(a2, y)\n",
      "Cell \u001b[1;32mIn[58], line 6\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(X, W1, W2)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# input layer\u001b[39;00m\n\u001b[0;32m      5\u001b[0m z0 \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m----> 6\u001b[0m z0 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mz0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias_row\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# for the bias \u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# hidden layer 1\u001b[39;00m\n\u001b[0;32m      9\u001b[0m z1 \u001b[38;5;241m=\u001b[39m W1\u001b[38;5;241m.\u001b[39mdot(z0)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\shape_base.py:296\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    295\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_units = 128\n",
    "\n",
    "# HE intialisation \n",
    "W1 = np.random.randn(hidden_units, 785) * np.sqrt(2.0 / 785) # note that to account for the bias we use (10, 785) instead of (10, 784)\n",
    "W2 = np.random.randn(10, hidden_units + 1) * np.sqrt(2.0 / 129) # note that to account for the bias we use (10, 11) instead of (10, 10)\n",
    "\n",
    "\n",
    "num_epochs = 10000\n",
    "\n",
    "W1, W2 = train(W1, W2, X, labels, val_X, val_labels, num_epochs, lr=0.01, smoothing_param=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28000 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0           0       0       0       0       0       0       0       0       0   \n",
       "1           0       0       0       0       0       0       0       0       0   \n",
       "2           0       0       0       0       0       0       0       0       0   \n",
       "3           0       0       0       0       0       0       0       0       0   \n",
       "4           0       0       0       0       0       0       0       0       0   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "27995       0       0       0       0       0       0       0       0       0   \n",
       "27996       0       0       0       0       0       0       0       0       0   \n",
       "27997       0       0       0       0       0       0       0       0       0   \n",
       "27998       0       0       0       0       0       0       0       0       0   \n",
       "27999       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "       pixel9  ...  pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0           0  ...         0         0         0         0         0   \n",
       "1           0  ...         0         0         0         0         0   \n",
       "2           0  ...         0         0         0         0         0   \n",
       "3           0  ...         0         0         0         0         0   \n",
       "4           0  ...         0         0         0         0         0   \n",
       "...       ...  ...       ...       ...       ...       ...       ...   \n",
       "27995       0  ...         0         0         0         0         0   \n",
       "27996       0  ...         0         0         0         0         0   \n",
       "27997       0  ...         0         0         0         0         0   \n",
       "27998       0  ...         0         0         0         0         0   \n",
       "27999       0  ...         0         0         0         0         0   \n",
       "\n",
       "       pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0             0         0         0         0         0  \n",
       "1             0         0         0         0         0  \n",
       "2             0         0         0         0         0  \n",
       "3             0         0         0         0         0  \n",
       "4             0         0         0         0         0  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "27995         0         0         0         0         0  \n",
       "27996         0         0         0         0         0  \n",
       "27997         0         0         0         0         0  \n",
       "27998         0         0         0         0         0  \n",
       "27999         0         0         0         0         0  \n",
       "\n",
       "[28000 rows x 784 columns]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['label'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_X \u001b[38;5;241m=\u001b[39m \u001b[43mtest_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_numpy()\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m \u001b[38;5;66;03m# normalising\u001b[39;00m\n\u001b[0;32m      2\u001b[0m test_labels \u001b[38;5;241m=\u001b[39m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy()\n",
      "File \u001b[1;32mc:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:5344\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5197\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5198\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5205\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5206\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5208\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5209\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5342\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5343\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5346\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5350\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5351\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5352\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:4711\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4709\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4710\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4711\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4714\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:4753\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4751\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4753\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4754\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4756\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4757\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\tanxe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:7000\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   6999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7000\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7001\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7002\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['label'] not found in axis\""
     ]
    }
   ],
   "source": [
    "test_X = test_df.drop('label', axis=1).to_numpy().T / 255.0 # normalising\n",
    "test_labels = test_df['label'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m predictions \u001b[38;5;241m=\u001b[39m predict(\u001b[43mtest_X\u001b[49m, W1, W2)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, accuracy(predictions, test_labels))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_X' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = predict(test_X, W1, W2)\n",
    "print('accuracy', accuracy(predictions, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding one more layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W1, W2, W3):\n",
    "    bias_row = np.ones(X.shape[1])\n",
    "\n",
    "    # input layer\n",
    "    z0 = X\n",
    "    z0 = np.vstack([z0, bias_row]) # for the bias \n",
    "\n",
    "    # hidden layer 1\n",
    "    z1 = W1.dot(z0)\n",
    "    a1 = ReLU(z1)\n",
    "    a1 = np.vstack([a1, bias_row]) # for the bias \n",
    "\n",
    "    # hidden layer 2\n",
    "    z2 = W2.dot(a1)\n",
    "    a2 = tanh(z2)\n",
    "    a2 = np.vstack([a2, bias_row]) # for the bias \n",
    "\n",
    "    # output layer\n",
    "    z3 = W3.dot(a2)\n",
    "    a3 = softmax(z3)\n",
    "    \n",
    "    return z0, z1, a1, z2, a2, z3, a3\n",
    "\n",
    "def backward(z0, W1, z1, a1, W2, z2, a2, W3, z3, a3, y):\n",
    "    n = y.shape[1] # number of samples\n",
    "\n",
    "    # output layer\n",
    "    dL_dz3 = dCE_dz(a3, y)\n",
    "    dL_dW3 = dL_dz3.dot(a2.T) / n\n",
    "\n",
    "    # hidden layer 2\n",
    "    dL_da2 = W3.T.dot(dL_dz3)\n",
    "    dL_da2 = dL_da2[:-1] # removing the bias row\n",
    "    da2_dz2 = dtanh_dz(z2)\n",
    "    dL_dz2 = dL_da2 * da2_dz2\n",
    "    dL_dW2 = dL_dz2.dot(a1.T) / n \n",
    "\n",
    "    # hidden layer 1\n",
    "    dL_da1 = W2.T.dot(dL_dz2)\n",
    "    dL_da1 = dL_da1[:-1] # removing the bias row\n",
    "    da1_dz1 = dReLU_dz(z1)\n",
    "    dL_dz1 = dL_da1 * da1_dz1\n",
    "    dL_dW1 = dL_dz1.dot(z0.T) / n\n",
    "\n",
    "    return dL_dW1, dL_dW2, dL_dW3\n",
    "\n",
    "def batch_grad_descent(W1, dL_dW1, W2, dL_dW2, W3, dL_dW3, lr):\n",
    "    W1 -= (lr * dL_dW1)\n",
    "    W2 -= (lr * dL_dW2)\n",
    "    W3 -= (lr * dL_dW3)\n",
    "    return W1, W2, W3\n",
    "\n",
    "def train(W1, W2, W3, X, labels, val_X, val_labels, num_epochs, lr=0.1, patience=10, smoothing_param=0):\n",
    "\n",
    "    original_patience = patience\n",
    "\n",
    "    # one-hot encode labels\n",
    "    y = oneHot(labels, smoothing_param=smoothing_param)\n",
    "    val_y = oneHot(val_labels)\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    # training loop\n",
    "    for i in range(num_epochs):\n",
    "\n",
    "        # forward propagation\n",
    "        z0, z1, a1, z2, a2, z3, a3 = forward(X, W1, W2, W3)\n",
    "\n",
    "        # print the training loss\n",
    "        training_loss = crossEntropyLoss(a3, y)\n",
    "\n",
    "        # validate\n",
    "        val_predictions = forward(val_X, W1, W2, W3)[-1]\n",
    "        validation_loss = crossEntropyLoss(val_predictions, val_y)\n",
    "\n",
    "        # print the training loss\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('epoch: ', i + 1)\n",
    "            print('     training loss: ', training_loss)\n",
    "            print('     validation loss: ', validation_loss)\n",
    "\n",
    "        # only save the model when it validation loss improves\n",
    "        if validation_loss <= best_val_loss:\n",
    "            best_val_loss = validation_loss\n",
    "            best_W1, best_W2, best_W3 = W1.copy(), W2.copy(), W3.copy()\n",
    "            patience = original_patience\n",
    "\n",
    "        else:\n",
    "            print(\"     Bullocks! I think I'm overfitting...\")\n",
    "            patience -= 1\n",
    "\n",
    "            # early stopping\n",
    "            if patience <= 0:\n",
    "                print(\"     I'm definitely overfitted. Stopping training...\")\n",
    "                break\n",
    "\n",
    "        # back propagation\n",
    "        dL_dW1, dL_dW2, dL_dW3 = backward(z0, W1, z1, a1, W2, z2, a2, W3, z3, a3, y)\n",
    "\n",
    "        # update weights\n",
    "        W1, W2, W3 = batch_grad_descent(W1, dL_dW1, W2, dL_dW2, W3, dL_dW3, lr)\n",
    "\n",
    "    return best_W1, best_W2, best_W3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m W3 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m10\u001b[39m, hidden_units \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m11\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m10\u001b[39m)) \u001b[38;5;66;03m# note that to account for the bias we use hidden_units + 1\u001b[39;00m\n\u001b[0;32m      8\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[1;32m----> 9\u001b[0m W1, W2, W3\u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmoothing_param\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[66], line 67\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(W1, W2, W3, X, labels, val_X, val_labels, num_epochs, lr, patience, smoothing_param)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# training loop\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     65\u001b[0m \n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# forward propagation\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m     z0, z1, a1, z2, a2, z3, a3 \u001b[38;5;241m=\u001b[39m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# print the training loss\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     training_loss \u001b[38;5;241m=\u001b[39m crossEntropyLoss(a3, y)\n",
      "Cell \u001b[1;32mIn[66], line 9\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(X, W1, W2, W3)\u001b[0m\n\u001b[0;32m      6\u001b[0m z0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([z0, bias_row]) \u001b[38;5;66;03m# for the bias \u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# hidden layer 1\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m z1 \u001b[38;5;241m=\u001b[39m \u001b[43mW1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m a1 \u001b[38;5;241m=\u001b[39m ReLU(z1)\n\u001b[0;32m     11\u001b[0m a1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([a1, bias_row]) \u001b[38;5;66;03m# for the bias \u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_units = 128\n",
    "\n",
    "# HE intialisation for ReLU layers and Glorot initialisation for tanh layers\n",
    "W1 = np.random.randn(hidden_units, 785) * np.sqrt(2.0 / 785) # note that to account for the bias we use 785 instead of 784\n",
    "W2 = np.random.randn(hidden_units, hidden_units + 1) * np.sqrt(2.0 / 129) # note that to account for the bias we use hidden_units + 1\n",
    "W3 = np.random.randn(10, hidden_units + 1) * np.sqrt(2.0 / (11 + 10)) # note that to account for the bias we use hidden_units + 1\n",
    "\n",
    "num_epochs = 10000\n",
    "W1, W2, W3= train(W1, W2, W3, X, labels, val_X, val_labels, num_epochs, lr=0.01, smoothing_param=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W1, W2, W3):\n",
    "    z0, z1, a1, z2, a2, z3, a3  = forward(X, W1, W2, W3)\n",
    "    return np.argmax(a3, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test_df.drop('label', axis=1).to_numpy().T / 255.0 # normalising\n",
    "test_labels = test_df['label'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9595238095238096\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(test_X, W1, W2, W3)\n",
    "print('accuracy', accuracy(predictions, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing the model off\n",
    "Let's randomly sample from the test dataset and see if the model can correctly classify the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's 7!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display the sample\n",
    "sample = test_df.sample(n=1, axis=0).to_numpy() \n",
    "image = sample.reshape((28, 28)) \n",
    "plt.imshow(image, cmap='gray_r', interpolation='nearest')\n",
    "sample = sample.T / 255 # normalise and transpose first\n",
    "\n",
    "# classify it\n",
    "print(f\"It's {predict(sample, W1, W2, W3)[0]}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on home made images\n",
    "we need to convert it to 28 by 28 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def convert_to_MNIST_format(image_path):\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # convert to grey scale\n",
    "    img = img.convert('L')\n",
    "\n",
    "    # resize to 20 by 20\n",
    "    img = img.resize((20, 20))\n",
    "\n",
    "    # create the background canvas\n",
    "    canvas = Image.new('L', (28, 28), 255)\n",
    "\n",
    "    # paste the original image on the canvas\n",
    "    canvas.paste(img, (4, 4))\n",
    "\n",
    "    # convert to numpy array\n",
    "    img_array = 255 - np.array(canvas)\n",
    "\n",
    "    # convert pixels with values less than 50 to white  \n",
    "    img_array[img_array < 55] = 0\n",
    "\n",
    "    # convert pixels with values more than 200 to black\n",
    "    img_array[img_array > 200] = 255\n",
    "\n",
    "\n",
    "    plt.imshow(img_array, cmap='gray_r')\n",
    "    return img_array\n",
    "\n",
    "hand_drawn_number = convert_to_MNIST_format('C:/Users/tanxe/Programming/ML/NN/handmade_numbers/funny_number_2.jpg')\n",
    "\n",
    "# convert it into the right format\n",
    "hand_drawn_number = np.array(hand_drawn_number).flatten()\n",
    "hand_drawn_number = np.array([hand_drawn_number]).T / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's an 7!\n"
     ]
    }
   ],
   "source": [
    "print(f\"It's an {predict(hand_drawn_number, W1, W2, W3)[0]}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = [W1, W2, W3]\n",
    "W_obj = np.array(W, dtype=object)  \n",
    "np.save('Image_Classifier_Weights_2_hidden_layers.npy', W_obj, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_loaded = np.load('Image_Classifier_Weights_2_hidden_layers.npy', allow_pickle=True)\n",
    "W1, W2, W3 = W_loaded.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV for kaggle submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28000"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_test_df = pd.read_csv('C:/Users/tanxe/Programming/ML/NN/data/test.csv')\n",
    "kaggle_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageId  Label\n",
       "0        1      2\n",
       "1        2      0\n",
       "2        3      9\n",
       "3        4      9\n",
       "4        5      3"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_test = kaggle_test_df.to_numpy().T / 255 \n",
    "kaggle_preds = predict(kaggle_test, W1, W2, W3)\n",
    "\n",
    "kaggle_submission_df = pd.DataFrame({\n",
    "    'ImageId': range(1, len(kaggle_preds) + 1),\n",
    "    'Label': kaggle_preds\n",
    "})\n",
    "\n",
    "kaggle_submission_df.to_csv('kaggle_submission.csv', index=False)\n",
    "kaggle_submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
