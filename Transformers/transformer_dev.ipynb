{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7797a0",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "We will build an English to German translator model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f021aef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "191e6816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa38aeb6",
   "metadata": {},
   "source": [
    "### Multi-head Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db974c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, 'd_model must be divisible by num_heads'\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        return output\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_len, _ = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.W_q(Q)\n",
    "        K = self.W_k(K)\n",
    "        V = self.W_v(V)\n",
    "\n",
    "        Q = self.split_heads(Q)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa0204b",
   "metadata": {},
   "source": [
    "### Position-wise Feed-Forward Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a0a33318",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # GELU tends to perform better but is more computationally expensive than ReLU\n",
    "        self.gelu = nn.GELU()\n",
    "        # self.relu = nn.ReLU()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.W1 = nn.Linear(d_model, d_ff, bias=True) \n",
    "        self.W2 = nn.Linear(d_ff, d_model, bias=True) \n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.norm(x)\n",
    "        x = self.W1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.W2(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1b3e5b",
   "metadata": {},
   "source": [
    "## Attention blocks\n",
    "The full attention block consists of LayerNorm, followed by Multi-Headed Attention layer, followed by residual connections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b777b0",
   "metadata": {},
   "source": [
    "### Self-Attention block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37398d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        residual = x\n",
    "        x = self.norm(x)\n",
    "        x = self.attn(x, x, x, mask)\n",
    "        x = self.dropout(x) \n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e26968e",
   "metadata": {},
   "source": [
    "### Cross-Attention block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8d3f079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, encoder_output, mask=None):\n",
    "        residual = x\n",
    "        x = self.norm(x)\n",
    "        x = self.attn(x, encoder_output, encoder_output, mask)\n",
    "        return residual + self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6766a1",
   "metadata": {},
   "source": [
    "## Decoder Layer and Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2adda8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = SelfAttentionBlock(d_model, num_heads, dropout)\n",
    "        self.cross_attn = CrossAttentionBlock(d_model, num_heads, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        x = self.self_attn(x, tgt_mask)\n",
    "        x = self.cross_attn(x, encoder_output, src_mask)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = SelfAttentionBlock(d_model, num_heads, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.self_attn(x, mask)\n",
    "        x = self.ffn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8558da0a",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a80f532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len):\n",
    "        super().__init__()\n",
    "        position = torch.arange(max_seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32a392e",
   "metadata": {},
   "source": [
    "## Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40f6407",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8, \n",
    "                 num_layers=6, d_ff=2048, max_seq_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder_embed = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embed = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "            \n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_len = tgt.size(1)\n",
    "        causal_mask = (1 - torch.triu(torch.ones(1, seq_len, seq_len), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & causal_mask.to(device)\n",
    "        return src_mask, tgt_mask\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        \n",
    "        src_emb = self.encoder_embed(src) * math.sqrt(d_model) \n",
    "        src_emb = self.pos_encoding(src_emb)\n",
    "        src_emb = self.dropout(src_emb)\n",
    "        \n",
    "        tgt_emb = self.decoder_embed(tgt) * math.sqrt(d_model) \n",
    "        tgt_emb =self.pos_encoding(tgt_emb)\n",
    "        tgt_emb = self.dropout(tgt_emb)\n",
    "        \n",
    "        enc_output = src_emb\n",
    "        for layer in self.encoder_layers:\n",
    "            enc_output = layer(enc_output, src_mask)\n",
    "            \n",
    "        dec_output = tgt_emb\n",
    "        for layer in self.decoder_layers:\n",
    "            dec_output = layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "            \n",
    "        return self.fc_out(dec_output)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a9c69",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6669b44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "train_dataset = load_dataset('iwslt2017', 'iwslt2017-en-de', trust_remote_code=True, split='train')\n",
    "val_dataset = load_dataset('iwslt2017', 'iwslt2017-en-de', trust_remote_code=True, split='validation')\n",
    "test_dataset = load_dataset('iwslt2017', 'iwslt2017-en-de', trust_remote_code=True, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "86ebd87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: Dataset({\n",
      "    features: ['translation'],\n",
      "    num_rows: 206112\n",
      "})\n",
      "Validation dataset: Dataset({\n",
      "    features: ['translation'],\n",
      "    num_rows: 888\n",
      "})\n",
      "Test dataset: Dataset({\n",
      "    features: ['translation'],\n",
      "    num_rows: 8079\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train dataset: {train_dataset}\")\n",
    "print(f\"Validation dataset: {val_dataset}\")\n",
    "print(f\"Test dataset: {test_dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c45b0252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    dataset = load_dataset('iwslt2017', 'iwslt2017-en-de')\n",
    "    train_df = pd.DataFrame(dataset['train']['translation'])\n",
    "    valid_df = pd.DataFrame(dataset['validation']['translation'])\n",
    "    test_df = pd.DataFrame(dataset['test']['translation'])\n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "train_df, val_df, test_df = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bf609ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>de</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vielen Dank, Chris.</td>\n",
       "      <td>Thank you so much, Chris.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Es ist mir wirklich eine Ehre, zweimal auf die...</td>\n",
       "      <td>And it's truly a great honor to have the oppor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ich bin wirklich begeistert von dieser Konfere...</td>\n",
       "      <td>I have been blown away by this conference, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Das meine ich ernst, teilweise deshalb -- weil...</td>\n",
       "      <td>And I say that sincerely, partly because  I ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Versetzen Sie sich mal in meine Lage!</td>\n",
       "      <td>Put yourselves in my position.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  de  \\\n",
       "0                                Vielen Dank, Chris.   \n",
       "1  Es ist mir wirklich eine Ehre, zweimal auf die...   \n",
       "2  Ich bin wirklich begeistert von dieser Konfere...   \n",
       "3  Das meine ich ernst, teilweise deshalb -- weil...   \n",
       "4              Versetzen Sie sich mal in meine Lage!   \n",
       "\n",
       "                                                  en  \n",
       "0                          Thank you so much, Chris.  \n",
       "1  And it's truly a great honor to have the oppor...  \n",
       "2  I have been blown away by this conference, and...  \n",
       "3  And I say that sincerely, partly because  I ne...  \n",
       "4                     Put yourselves in my position.  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "37c42ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.data.iloc[idx]['en']\n",
    "        tgt_text = self.data.iloc[idx]['de']\n",
    "        \n",
    "        src_enc = self.tokenizer(\n",
    "            src_text, \n",
    "            max_length=self.max_len, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        tgt_enc = self.tokenizer(\n",
    "            tgt_text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'src_ids': src_enc['input_ids'].squeeze(),\n",
    "            'tgt_ids': tgt_enc['input_ids'].squeeze(),\n",
    "            'src_mask': src_enc['attention_mask'].squeeze(),\n",
    "            'tgt_mask': tgt_enc['attention_mask'].squeeze()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2749fee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1ccc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size=32):\n",
    "    dataset = load_dataset('iwslt2017', 'iwslt2017-en-de')\n",
    "    tokenizer = AutoTokenizer.from_pretrained('t5-small')\n",
    "    \n",
    "    train_df = pd.DataFrame(dataset['train']['translation'])\n",
    "    val_df = pd.DataFrame(dataset['validation']['translation'])\n",
    "    test_df = pd.DataFrame(dataset['test']['translation'])\n",
    "    \n",
    "    # train_df['de'] = '<s> ' + train_df['de'] + ' </s>'\n",
    "    # val_df['de'] = '<s> ' + val_df['de'] + ' </s>'\n",
    "    # test_df['de'] = '<s> ' + test_df['de'] + ' </s>'\n",
    "    \n",
    "    train_ds = TranslationDataset(train_df, tokenizer, 128)\n",
    "    val_ds = TranslationDataset(val_df, tokenizer, 128)\n",
    "    test_ds = TranslationDataset(test_df, tokenizer, 128)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb6f91f",
   "metadata": {},
   "source": [
    "## Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6623e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Batch 100, Train Loss: nan\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-5):\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    i=1\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            src_ids = batch['src_ids'].to(device)\n",
    "            tgt_ids = batch['tgt_ids'].to(device)\n",
    "            \n",
    "            outputs = model(src_ids, tgt_ids[:, :-1])\n",
    "            loss = criterion(outputs.contiguous().view(-1, outputs.size(-1)), \n",
    "                            tgt_ids[:, 1:].contiguous().view(-1))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(f'     Batch {i}, Train Loss: {loss.item()}')\n",
    "            i += 1\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss/len(train_loader):.4f}')\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                src_ids = batch['src_ids'].to(device)\n",
    "                tgt_ids = batch['tgt_ids'].to(device)\n",
    "                \n",
    "                outputs = model(src_ids, tgt_ids[:, :-1])\n",
    "                loss = criterion(outputs.contiguous().view(-1, outputs.size(-1)), \n",
    "                                tgt_ids[:, 1:].contiguous().view(-1))\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        print(f'Validation Loss: {val_loss/len(val_loader):.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69751a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main training loop\n",
    "if __name__ == '__main__':\n",
    "    train_loader, val_loader, _, tokenizer = load_data()\n",
    "    \n",
    "    model = Transformer(\n",
    "        src_vocab_size=tokenizer.vocab_size,\n",
    "        tgt_vocab_size=tokenizer.vocab_size,\n",
    "        d_model=512,\n",
    "        num_heads=8,\n",
    "        num_layers=6,\n",
    "        d_ff=2048,\n",
    "        max_seq_len=128\n",
    "    ).to(device)\n",
    "    \n",
    "    train_model(model, train_loader, val_loader)\n",
    "        \n",
    "    torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, \n",
    "    './translator.pth')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
