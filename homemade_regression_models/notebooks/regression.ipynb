{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3847698",
   "metadata": {},
   "source": [
    "## linear Regression\n",
    "We start this project with a linear regression model. After training this model, it should be able to give accurate predictions(labels), given a set of inputs(features). Take note that the initial list of weights w and the bias, b, can start of as anything. We will initialise it with all its entries as 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f126b44",
   "metadata": {},
   "source": [
    " We first need to import the relavent modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb07fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn\n",
    "import pprint\n",
    "import math\n",
    "import csv\n",
    "import random\n",
    "print = pprint.pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2077f5a6",
   "metadata": {},
   "source": [
    "Suppose we have some datapoints and we wish to fit a best-fit line $\\hat{y} = w_1 x_{1} + w_2 x_{2} + \\dots + w_m x_{m} + b$. Let's say $\\hat{y}_i$ is the predicted output value for each point $(x_{i1}, ..., x_{im})$, given by $\\hat{y}_i = w_1 x_{i1} + w_2 x_{i2} + \\dots + w_m x_{im} + b$.  \n",
    "If we let\n",
    "$$\n",
    "\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "x_{11} & x_{12} & \\cdots & x_{1m} & 1 \\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2m} & 1 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "x_{n1} & x_{n2} & \\cdots & x_{nm} & 1\n",
    "\\end{bmatrix}\n",
    ",\n",
    "\\mathbf{w} = \\begin{bmatrix}\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "\\vdots \\\\\n",
    "w_m \\\\\n",
    "b\n",
    "\\end{bmatrix}\n",
    ",\n",
    "\\mathbf{Y} = \\begin{bmatrix}\n",
    "\\hat{y}_1 \\\\\n",
    "\\hat{y}_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{y}_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "we can write\n",
    "\n",
    "$$\n",
    "\\mathbf{Y} = \\mathbf{X} \\mathbf{w}\n",
    "$$\n",
    "\n",
    "Now, that's a dirty little trick.\n",
    "\n",
    "Suppose we fit line, how then do we know how \"good\" our line is? We consider the residual sum of squares (or the sum of squared errors), $RSS$: \n",
    "\n",
    "$$\n",
    "RSS = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "where $y_i$ is the actual output value.\n",
    "\n",
    "A large $RSS$ indicates a poorly fitted curve. Hence in order to properly fit a curve, we want to minimize $RSS$ by adjusting our weights, and we can do this with gradient descent. The idea of gradient descent is to \"move around\" in the solution space until we find a minimum (praying that it is close to the global minimum). So then, how do we know which direction to move in? We consider the gradient matrix, $\\nabla S$:\n",
    "\n",
    "$$\n",
    "\\nabla RSS = \\begin{bmatrix} \\frac{\\partial (RSS)}{\\partial w_1} \\\\ \\frac{\\partial (RSS)}{\\partial w_2} \\\\ \\vdots \\\\ \\frac{\\partial (RSS)}{\\partial w_m} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$\\nabla RSS$ gives us the direction to move in. So, taking $\\mathbf{w} - lr * \\nabla RSS$ essentially lets us descend closer to the minimum. Also, the adjustable coeffient $lr$ is there to help prevent us from exceeding the speed limit and overshooting the minimum as we descend. All thats left now is to find a way to calculate $\\nabla RSS$.\n",
    "\n",
    "Take note that the partial derivative of each weight with respect to the sum of squared residuals is given by:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial (RSS)}{\\partial w_k} = -2 \\sum_{i=1}^{n} (y_i - \\hat{y}_i) \\cdot x_{ik}\n",
    "$$\n",
    "\n",
    "Iterating over all the weights to find each partial derivative, would be so troublesome and messy though, but I am not done with my dirty little tricks. Let's use multiplication again:\n",
    "\n",
    "$$\n",
    "\\nabla RSS = \\mathbf{X}^T (\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{bmatrix} - \\mathbf{Y}) \n",
    "$$\n",
    "\n",
    "Now all we have to do is implement it. Thankfully numpy exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54ebcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lin_reg_model:\n",
    "    \n",
    "    def __init__(self, name, weights, steps, lr):\n",
    "        self.name = name\n",
    "\n",
    "        # ensure that steps, lr and the weights are of the right data type\n",
    "        if type(steps) != int:\n",
    "            raise TypeError(\"steps must be an int!\")\n",
    "        if (type(lr) != float) and (type(lr) != int):\n",
    "            raise TypeError(\"learning rate must be an int or float!\")\n",
    "        if type(weights) != np.ndarray:\n",
    "            raise TypeError(\"weights must be a matrix\")\n",
    "        \n",
    "        self.weights = weights.astype(float) # defining the attributes\n",
    "        self.steps = steps\n",
    "        self.lr = lr\n",
    "\n",
    "    \n",
    "    #-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "    #-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "    \n",
    "    # defining our prediction function (y = w1*x1 + w2*x2 + ... + wn*xn)\n",
    "    # note that w is a n by 1 matrix that contains our n weights\n",
    "    def predict(self, X):\n",
    "        if not all(isinstance(item, list) for item in X): # this ensures that we can enter a single input as a single list\n",
    "            X.append(1)\n",
    "            X = np.array(X)\n",
    "            return float(np.matmul(X, self.weights)) \n",
    "        else: # if we are considering multiple inputs its a different story\n",
    "            X = np.array(X) \n",
    "            num_rows = np.shape(X)[0] # getting the number of columns\n",
    "            print(np.array([ [1] for i in range(num_rows)]))\n",
    "            X = np.append(X, np.array([ [1] for i in range(num_rows)]), axis = 1) \n",
    "            return [float(i) for i in np.matmul(X, self.weights)] # convert each value in the matrix to a float before returning\n",
    "\n",
    "    # defining a prediction function for internal calculations only\n",
    "    def yhat(self, X):\n",
    "        return np.matmul(X, self.weights) \n",
    "    \n",
    "    # defining a function to calculate residuals\n",
    "    def res(self, datapoints):\n",
    "        X, Y = datapoints\n",
    "        return Y - self.yhat(X) \n",
    "\n",
    "    # defining a function to find the sum of the square of the residuals, which we abbreviated as RSS\n",
    "    def RSS(self, datapoints):\n",
    "        return sum(self.res(datapoints) ** 2) \n",
    "\n",
    "    # defining a function for the root mean square error\n",
    "    def rmse(self, datapoints):\n",
    "        num_of_points = np.shape(datapoints[0])[1] # finding the number of datapoints by finding the number of rows\n",
    "        return math.sqrt(self.RSS(datapoints) / num_of_points) \n",
    "    \n",
    "    # defining a function dRSS to find the partial derivative of RSS with respect to each weight in self.weights\n",
    "    # the values of the partial derivatives will be stored in a n by 1 matrix\n",
    "    def dRSS_dw(self, datapoints):\n",
    "        X, Y = datapoints\n",
    "        return -2 * np.matmul(X.transpose(), self.res(datapoints)) # matrix multiplication can get us the partial derivatives easily\n",
    "\n",
    "    def improve(self, datapoints):\n",
    "        return self.weights - self.lr * self.dRSS_dw(datapoints) # return the improved self.weights\n",
    "\n",
    "    # defining a function to train the model by repeating the improve function for each training datapoint\n",
    "    # steps is the number of iterations and lr is the learning rate\n",
    "    def train(self, training_datapoints, testing_datapoints = None):\n",
    "        if testing_datapoints == None:\n",
    "            testing_datapoints = training_datapoints\n",
    "\n",
    "        print(f\"the current root mean square error is {self.rmse(testing_datapoints)}\")\n",
    "        print(\"training model...\")\n",
    "        for i in range(self.steps): \n",
    "            self.weights = self.improve(training_datapoints) # set self.weights to the improved self.weights\n",
    "        # print(self.weights)\n",
    "        print(\"model trained!\")\n",
    "        print(f\"the root mean square error is now {self.rmse(testing_datapoints)}\") # seeing our improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92040f8",
   "metadata": {},
   "source": [
    "## Data Munging\n",
    "Before feeding the data to the linear regression model, we need it to be in a more usable format. Ideally, we want 2 lists, one for training and one for testing, where each element in each list is a datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df77c0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mung(filename):\n",
    "#     with open(filename) as f:\n",
    "#         data = f.read().split(\"\\n\") # split each line into individual elements in the training data list\n",
    "#         data.remove(\"\") # the last element of training data is \"\"\n",
    "\n",
    "#         X = [ data.split(\",\")[:-1] + [1] for data in data ] # extract the X values\n",
    "#         Y = [ data.split(\",\")[-1] for data in data ] # extract the Y values\n",
    "        \n",
    "#         X.remove(X[0]) # purifying the data by removing the headers\n",
    "#         Y.remove(Y[0]) \n",
    "        \n",
    "#         X = np.array([ [ float(i) for i in j] for j in X ]) # converting all entries to floats\n",
    "#         Y = np.array([ float(i) for i in Y])\n",
    "\n",
    "#         B = np.array([ [1] for i in range(np.shape(X)[0]) ]) # creating a column of 1s for the bias\n",
    "\n",
    "#         return (X, Y)\n",
    "    \n",
    "# data must be in the form where the header of the csv file is \"x1, x2, ..., x3, y\"\n",
    "def mung(filename):\n",
    "    with open(filename) as f:\n",
    "        data = [ datapoint.split(\",\") for datapoint in f.read().split(\"\\n\")] \n",
    "        \n",
    "        data.remove(data[-1])\n",
    "       \n",
    "        X = [ datapoint[:-1] + [\"1\"] for datapoint in data] # note here that we add an additional \"1\" to account for the bias\n",
    "        Y = [ datapoint[-1] for datapoint in data ] \n",
    "        \n",
    "        X.remove(X[0])\n",
    "        Y.remove(Y[0])\n",
    "\n",
    "        X = np.array(X).astype(float)\n",
    "        Y = np.array(Y).astype(float)\n",
    "\n",
    "        # print(cat_0)\n",
    "        # print(cat_1)    \n",
    "\n",
    "        return (X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad0e056",
   "metadata": {},
   "source": [
    "## Linear Regression in Action\n",
    "To test the model above, we need some training data and testing, so we will use pizza data from [pro](https://www.progml.com/) as training data. As for testing data, ChaptGpt will generate it for us. Our model will aim to predict how many pizzas(Y) need to be made based on factors(X) such as Reservations, Temperature, Tourists, Pizzas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26498e06",
   "metadata": {},
   "source": [
    "We start by munging the data from the csv files with our mung function and instantiating our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e3b4cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([1, 1, 1, 1]) # defining our weights\n",
    "pizza_forecast = lin_reg_model(\"pizza_forecast\", w, 10000, 0.00001)\n",
    "training_datapoints = mung(\"../data/pizza_training.csv\") \n",
    "testing_datapoints = mung(\"../data/pizza_testing.csv\")\n",
    "\n",
    "# print(training_datapoints)\n",
    "# print(testing_datapoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e3a370",
   "metadata": {},
   "source": [
    "Now we are ready to train the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34548a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'the current root mean square error is 21.644860821913362'\n",
      "'training model...'\n",
      "'model trained!'\n",
      "'the root mean square error is now 7.11490373107488'\n"
     ]
    }
   ],
   "source": [
    "pizza_forecast.train(training_datapoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c5ecce",
   "metadata": {},
   "source": [
    "We can see above that there is a huge imrpovement in the root mean square error, so our model seems to be working. Finally, the moment we have been waiting for: Our pizza_forecast machine is ready! Let's do some predicting, with values from the testing dataset. Lets's say today we have 11 reservations, and the temperature is 22 degress. Furthermore, there are 8 tourists in town. How many pizzas should we make today? (The correct value is )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7fdb908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.572885875540585"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pizza_forecast.predict([11, 22, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b975ff7a",
   "metadata": {},
   "source": [
    "The correct value is 43 so we did infact get pretty close. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf49083a",
   "metadata": {},
   "source": [
    "## Logisting Regression\n",
    "Now that we are done, lets build a classifier, with logistic regression. Suppose we have 2 categories, A and B. Now let's say we are given an object, and a set of inputs pertaining to that object, and we want to predict the category the object is in. Clearly, our linear regression model above won't work, so we need some new toys.\n",
    "\n",
    "What we want is a model that gives us outputs between 0 and 1. For example, an output greater than a threshold value, say 0.5, predicts that the object is likely in category A while an output less than the threshold value could predict that the object is likely in B. One way to achieve this is to scale our linear regression line to a value between 0 and 1 using the sigmoid function:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Notice that $0 < \\sigma(x) < 1$ for all real numbers x. If we simply consider $\\sigma(y)$ where $y$ is our predicted output value, if we were to feed it to the regression model, magic happens! \n",
    "\n",
    "$$\n",
    "\\sigma(x_1, \\dots, x_k) = \\frac{1}{1 + e^{-w_1 x_1 + \\dots + w_kx_k + b}}\n",
    "$$\n",
    "\n",
    "where $x_1, \\dots, x_k$ are our model's inputs. \n",
    "\n",
    "An important thing to note is that if we are using the sigmoid function, (as you might have found out the hard way,) we cannot use our familiar sum of square residuals to measure the error. This is because it will give a curve that gradient descent won't like i,e. not a very smooth one.\n",
    "\n",
    "So instead of trying to minimise the sum of squared errors, we will use the maximum log of likelihood estimation (MLE). What this does is it finds the sigmoid curve that gives us the greatest probability of observing our training data i.e the likelihood. Say we have $n$ of our datapoints predicting 1s and $m$ predicting 0s. Furthermore, supposed that when the actual outputs are 1 and 0, the model outputs $p_j \\,\\text{for some} \\, j \\in \\{1, 2, \\dots, n\\}$ and $q_k \\,\\text{for some} \\, k \\in \\{1, 2, \\dots, m\\}$ respectively. Then, the likelyhood, $L$, is given by:\n",
    "\n",
    "$$\n",
    "L = \\prod_{i=1}^{m} \\hat{y}_i \\prod_{i=m+1}^{n} (1 - \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "However, to we want to make use of gradient descent (or ascent in this case), so we need to find a gradient matrix. However, L is not well behaved and differentiating him could lead to the destruction of our planet. Hence we consider the log of the likelihood, $\\ell = log(L)$. This is is nice because it turns $L$ into a summation (instead of a multiplication), which is much easier to differentiate. Also, the set of weights that give the maximum $\\ell$ also give the maximum $L$. Hence we will use the log of the likelihood:\n",
    "\n",
    "$$\n",
    "\\ell = \\sum_{i=1}^{m} \\log(\\hat{y_i}) + \\sum_{i=m+1}^{n} \\log(1 - \\hat{y_i}) \n",
    "$$\n",
    "\n",
    "However, if we rewrite $\\ell$ as $\\ell= \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y_i}) + (1 - y_i) \\log(1 - \\hat{y_i}) \\right]$, it will be much easier to calculate.\n",
    "\n",
    "Now, if we take the partial derivative with respect to some weights, say $w_i$, we get:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell}{\\partial w_i} = \\sum_{i=1}^{n} \\left[x_i \\left( y_i - \\hat{y_i} \\right) \\right]\n",
    "$$\n",
    "\n",
    "where $y_i$ is the actual value for each datapoint. \n",
    "\n",
    "Wow, it simplified very nicely. It looks familiar, doesn't it? Yes, we can use matrices to express the gradient matrix, $\\nabla \\ell$ as:\n",
    "\n",
    "$$\n",
    "\\nabla \\ell = \\mathbf{X}^T (\\mathbf{Y} - \\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{bmatrix})\n",
    "$$\n",
    "\n",
    "where the definitions of $\\mathbf{Y}$ and $\\mathbf{X}^T$ are identical to the above definitions.\n",
    "\n",
    "Before we start, because we are finding the $maximum$ likelihood, we need to acscend. So we take $\\mathbf{w} + lr * \\nabla \\ell$ (instead of $\\mathbf{w} - lr * \\nabla \\ell$ like above).\n",
    "\n",
    "Now we can start writing code. We will start by creating a new class that'll inherit some of the functionality from lin_reg_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "305679da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class log_reg_model(lin_reg_model):\n",
    "    def __init__(self, name, weights, steps, lr, threshold = 0.5):\n",
    "        super().__init__(name, weights, steps, lr)\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    \n",
    "    #-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "    #-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "    # defining a sigmoid function\n",
    "    def __sigma(self, X):\n",
    "        return (1 + np.exp(-X)) ** -1\n",
    "\n",
    "    # defining our prediction function sigma(x)\n",
    "    def predict(self, X, show_yhat = False, add_weight = True):\n",
    "        if not all(isinstance(item, list) for item in X) and not all(isinstance(item, type(np.array([0]))) for item in X): # this ensures that we can enter a single input as a single list\n",
    "            if add_weight:\n",
    "                X+=[1]\n",
    "            X = np.array(X)\n",
    "            Y = np.matmul(X, self.weights)\n",
    "            # print(np.where(self.__sigma(Y) >= self.threshold, 1, 0))\n",
    "\n",
    "            if show_yhat:\n",
    "                print(f\"yhat: {self.__sigma(Y)}\")\n",
    "                \n",
    "            return int(np.where(self.__sigma(Y) >= self.threshold, 1, 0))\n",
    "        else: # if we are considering multiple inputs its a different story\n",
    "            if add_weight:\n",
    "                X = np.array(X) \n",
    "                num_rows = np.shape(X)[0] # getting the number of columns\n",
    "                # print(np.array([ [1] for i in range(num_rows)]))\n",
    "                X = np.append(X, np.array([ [1] for i in range(num_rows)]), axis = 1) \n",
    "                \n",
    "            Y = np.matmul(X, self.weights)\n",
    "\n",
    "            if show_yhat:\n",
    "                print(f\"yhat: {self.__sigma(Y)}\")\n",
    "\n",
    "            return np.where(self.__sigma(Y) >= self.threshold, 1, 0) \n",
    "\n",
    "    # defining a prediction function for internal calculations only\n",
    "    def yhat(self, X, clip = True):\n",
    "        Y = np.matmul(X, self.weights)\n",
    "        if clip:\n",
    "            return np.clip(self.__sigma(Y), 0.000001, 0.999999) # this is to prevent log of numbers very close to 0\n",
    "        else:\n",
    "            return self.__sigma(Y)\n",
    "\n",
    "    # defining a function to measure level of log of the likelihood\n",
    "    def log_likelihood(self, datapoints):\n",
    "        \n",
    "        X, Y = datapoints\n",
    "         \n",
    "        log_y = np.log(self.yhat(X))\n",
    "        log_1_y = np.log(1 - self.yhat(X))\n",
    "\n",
    "        return np.sum(Y * log_y + (1 - Y) * log_1_y)\n",
    "\n",
    "    # defining a function to measure the percentage of correct predictions\n",
    "    def percentage_correct(self, datapoints):\n",
    "        X, Y =  datapoints\n",
    "        \n",
    "        num_correct = sum(np.equal(Y, self.predict(X, add_weight = False)))\n",
    "        return num_correct/X.shape[0]\n",
    "\n",
    "    # defining a function to calculate the gradient matrix for log likelyhood\n",
    "    def dl_dw(self, datapoints):\n",
    "        X, Y = datapoints\n",
    "        return np.matmul(np.transpose(X), Y - self.yhat(X))\n",
    "\n",
    "    def improve(self, datapoints):\n",
    "        return self.weights + self.lr * self.dl_dw(datapoints)\n",
    "\n",
    "    # defining a function to train the model by repeating the improve function for each training datapoint\n",
    "    # steps is the number of iterations and lr is the learning rate\n",
    "    def train(self, training_datapoints, testing_datapoints = None):\n",
    "        if testing_datapoints == None:\n",
    "            testing_datapoints = training_datapoints\n",
    "\n",
    "        print(f\"the current log likelihood is {self.log_likelihood(testing_datapoints)}\")\n",
    "        print(\"training model...\")\n",
    "\n",
    "        for i in range(self.steps): \n",
    "            self.weights = self.improve(training_datapoints) # set self.weights to the improved self.weights\n",
    "            # print(self.log_likelihood(training_datapoints))\n",
    "        # print(self.weights)\n",
    "        print(\"model trained!\")\n",
    "        print(f\"the log likelihood is now {self.log_likelihood(testing_datapoints)}\") # seeing our improvements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1036e48c",
   "metadata": {},
   "source": [
    "## Logistic Regression in action\n",
    "To see if our logistic regression is successful, we will ask chatgpt to generate a well-behaved heart disease dataset. Based on a variety of factors like age, sex, blood pressure, cholesterol, and max heart rate, this model aims to predict whether an individual is at risk for heart disease.\n",
    "\n",
    "If we were to use a real dataset, we would first have to clean it and have some way of dealing with outliers. Furthermore, there may be many more unseen factors not recorded in the dataset, which could greatly affect the results of our logistic regression. However, handling these potential problems is out of the scope of this project, so we will ask chatgpt for a synthetic data set to test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16076dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_datapoints = mung(\"../data/heart_disease_training.csv\") \n",
    "# print(training_datapoints)\n",
    "w = [ 1 for i in training_datapoints[0][0] ] # the number of inputs is the number of weights hence we just use the first input as reference\n",
    "w = np.array(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ceca7b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifier = log_reg_model(\"classifier\", w, 100000, 0.000001, 0.5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f940dcca",
   "metadata": {},
   "source": [
    "Now we are finally ready to train the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f414549d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'the current log likelihood is -773.6686342444104'\n",
      "'training model...'\n",
      "'model trained!'\n",
      "'the log likelihood is now -27.042090189966498'\n"
     ]
    }
   ],
   "source": [
    "classifier.train(training_datapoints) # train the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4f214a",
   "metadata": {},
   "source": [
    "However, we are faced with one more issue: how do we decide on the threshold value?\n",
    "\n",
    "To truly obtain the best threshold value, we will need to look at the distribution, but I am lazy, so I'll just use a threshold value of 0.5.\n",
    "\n",
    "Let's see what percentage of the testing data we get correct..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7b465e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Percentage correct without logistic regression:0.75'\n",
      "'Percentage correct without logistic regression:0.55'\n"
     ]
    }
   ],
   "source": [
    "testing_datapoints = mung(\"../data/heart_disease_testing.csv\")\n",
    "\n",
    "print(f\"Percentage correct without logistic regression:{classifier.percentage_correct(testing_datapoints)}\")\n",
    "\n",
    "classifier.weights = [ 1 for i in training_datapoints[0][0] ] # weights here are not improved\n",
    "print(f\"Percentage correct without logistic regression:{classifier.percentage_correct(testing_datapoints)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb79ee8",
   "metadata": {},
   "source": [
    "Alright it's not great but there was an improvement. However, you could probably get better results with another threshold value. \n",
    "\n",
    "At least we know our logistic regression is probably working."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
